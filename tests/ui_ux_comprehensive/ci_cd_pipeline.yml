name: Comprehensive UI/UX Test Pipeline

on:
  push:
    branches: [ main, develop, feature/ui-ux-* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.12'
  NODE_VERSION: '18'
  PLAYWRIGHT_VERSION: 'latest'

jobs:
  setup-test-environment:
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.test-matrix.outputs.matrix }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
      
      - name: Install Playwright
        run: |
          pip install playwright pytest-playwright
          playwright install --with-deps
      
      - name: Generate test matrix
        id: test-matrix
        run: |
          python tests/ui_ux_comprehensive/generate_test_matrix.py

  # Parallel Test Streams (Phase 4 Implementation)
  unit-tests:
    runs-on: ubuntu-latest
    needs: setup-test-environment
    strategy:
      matrix:
        test-category: [
          'animation-engine',
          'rendering-system', 
          'performance-monitor',
          'adaptive-interface',
          'memory-optimizer',
          'terminal-optimizer'
        ]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
      
      - name: Run unit tests with coverage
        run: |
          python -m pytest tests/ui_ux_comprehensive/unit/ \
            --cov=app/cli \
            --cov-report=json \
            --cov-report=html \
            --junit-xml=test-results/unit-${{ matrix.test-category }}.xml \
            -v \
            --tb=short \
            --benchmark-json=benchmark-results/unit-${{ matrix.test-category }}.json \
            -k "${{ matrix.test-category }}" \
            --maxfail=5
      
      - name: Validate 60fps performance targets
        run: |
          python tests/ui_ux_comprehensive/validate_performance_targets.py \
            --benchmark-file benchmark-results/unit-${{ matrix.test-category }}.json \
            --fps-target 60 \
            --memory-limit-mb 200
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: unit-test-results-${{ matrix.test-category }}
          path: |
            test-results/
            benchmark-results/
            htmlcov/
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.json
          flags: unit-tests-${{ matrix.test-category }}

  performance-tests:
    runs-on: ubuntu-latest
    needs: setup-test-environment
    strategy:
      matrix:
        performance-category: [
          '60fps-validation',
          'memory-usage',
          'animation-smoothness',
          'rendering-efficiency',
          'virtual-scroll-performance'
        ]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies with performance tools
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
          pip install memory-profiler psutil pytest-benchmark
          sudo apt-get update
          sudo apt-get install -y htop iotop
      
      - name: Run performance tests
        run: |
          python -m pytest tests/ui_ux_comprehensive/performance/ \
            --benchmark-json=performance-results/${{ matrix.performance-category }}.json \
            --junit-xml=test-results/performance-${{ matrix.performance-category }}.xml \
            -v \
            -k "${{ matrix.performance-category }}" \
            --tb=line \
            --maxfail=3
        env:
          PERFORMANCE_TESTING: true
          MEMORY_LIMIT_MB: 200
          TARGET_FPS: 60
      
      - name: Analyze performance results
        run: |
          python tests/ui_ux_comprehensive/analyze_performance.py \
            --results-file performance-results/${{ matrix.performance-category }}.json \
            --generate-report
      
      - name: Check performance regression
        run: |
          python tests/ui_ux_comprehensive/check_performance_regression.py \
            --current performance-results/${{ matrix.performance-category }}.json \
            --baseline performance-baselines/${{ matrix.performance-category }}-baseline.json \
            --threshold 10
      
      - name: Upload performance results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-results-${{ matrix.performance-category }}
          path: |
            performance-results/
            test-results/

  cross-platform-tests:
    runs-on: ${{ matrix.os }}
    needs: setup-test-environment
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        terminal: [
          'default',
          'xterm', 
          'tmux',
          'screen'
        ]
        exclude:
          # Windows doesn't have tmux/screen by default
          - os: windows-latest
            terminal: tmux
          - os: windows-latest
            terminal: screen
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install OS-specific dependencies
        run: |
          if [ "${{ matrix.os }}" == "ubuntu-latest" ]; then
            sudo apt-get update
            sudo apt-get install -y tmux screen xterm
          elif [ "${{ matrix.os }}" == "macos-latest" ]; then
            brew install tmux screen
          fi
        shell: bash
      
      - name: Install Python dependencies
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
      
      - name: Run cross-platform tests
        run: |
          python -m pytest tests/ui_ux_comprehensive/cross_platform/ \
            --terminal=${{ matrix.terminal }} \
            --junit-xml=test-results/cross-platform-${{ matrix.os }}-${{ matrix.terminal }}.xml \
            -v \
            --tb=short \
            --maxfail=5
        env:
          TERMINAL_TYPE: ${{ matrix.terminal }}
          PLATFORM: ${{ matrix.os }}
      
      - name: Test Unicode support
        run: |
          python tests/ui_ux_comprehensive/test_unicode_support.py \
            --terminal=${{ matrix.terminal }} \
            --platform=${{ matrix.os }}
      
      - name: Upload cross-platform results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: cross-platform-results-${{ matrix.os }}-${{ matrix.terminal }}
          path: test-results/

  accessibility-tests:
    runs-on: ubuntu-latest
    needs: setup-test-environment
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
          pip install accessibility-checker axe-core-python
      
      - name: Install accessibility tools
        run: |
          sudo apt-get update
          sudo apt-get install -y espeak espeak-data
          # Install NVDA simulator for testing
          wget -q https://github.com/nvaccess/nvda/releases/latest/download/nvda.exe -O /tmp/nvda.exe || true
      
      - name: Run accessibility tests
        run: |
          python -m pytest tests/ui_ux_comprehensive/accessibility/ \
            --junit-xml=test-results/accessibility-tests.xml \
            -v \
            --tb=short \
            --maxfail=3
        env:
          ACCESSIBILITY_TESTING: true
      
      - name: Run WCAG compliance check
        run: |
          python tests/ui_ux_comprehensive/accessibility/wcag_compliance_check.py \
            --level AA \
            --generate-report
      
      - name: Test screen reader compatibility
        run: |
          python tests/ui_ux_comprehensive/accessibility/test_screen_readers.py \
            --simulate-nvda \
            --simulate-jaws \
            --generate-report
      
      - name: Upload accessibility results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accessibility-test-results
          path: |
            test-results/
            accessibility-reports/

  web-interface-tests:
    runs-on: ubuntu-latest
    needs: setup-test-environment
    strategy:
      matrix:
        browser: [chromium, firefox, webkit]
        test-suite: [
          'browser-compatibility',
          'websocket-functionality',
          'responsive-design',
          'performance-metrics'
        ]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
          pip install playwright pytest-playwright websockets
      
      - name: Install Playwright browsers
        run: playwright install --with-deps ${{ matrix.browser }}
      
      - name: Start CLIX web server
        run: |
          npm install
          npm run build
          npm start &
          sleep 10
          curl -f http://localhost:3000 || exit 1
        working-directory: ./web-interface
      
      - name: Run web interface tests
        run: |
          python -m pytest tests/ui_ux_comprehensive/web_interface/ \
            --browser=${{ matrix.browser }} \
            --junit-xml=test-results/web-${{ matrix.browser }}-${{ matrix.test-suite }}.xml \
            -v \
            -k "${{ matrix.test-suite }}" \
            --screenshot=on \
            --video=on \
            --tb=short
        env:
          BROWSER: ${{ matrix.browser }}
          WEB_INTERFACE_URL: http://localhost:3000
      
      - name: Test WebSocket connections
        run: |
          python tests/ui_ux_comprehensive/web_interface/test_websocket_stress.py \
            --max-connections 100 \
            --duration 60
      
      - name: Upload web test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: web-test-results-${{ matrix.browser }}-${{ matrix.test-suite }}
          path: |
            test-results/
            test-results-screenshots/
            test-results-videos/

  ai-intelligence-tests:
    runs-on: ubuntu-latest
    needs: setup-test-environment
    strategy:
      matrix:
        ai-component: [
          'ml-model-accuracy',
          'behavior-prediction',
          'adaptation-system',
          'tensorflowjs-integration'
        ]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install ML dependencies
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
          pip install tensorflow tensorflowjs numpy scikit-learn
      
      - name: Download pre-trained models
        run: |
          python tests/ui_ux_comprehensive/download_test_models.py \
            --model-dir test-models/
      
      - name: Run AI intelligence tests
        run: |
          python -m pytest tests/ui_ux_comprehensive/ai_intelligence/ \
            --junit-xml=test-results/ai-${{ matrix.ai-component }}.xml \
            -v \
            -k "${{ matrix.ai-component }}" \
            --tb=short \
            --maxfail=3
        env:
          MODEL_ACCURACY_THRESHOLD: 0.85
          PREDICTION_LATENCY_MS: 50
          AI_TESTING: true
      
      - name: Validate ML model performance
        run: |
          python tests/ui_ux_comprehensive/validate_ml_performance.py \
            --accuracy-threshold 0.85 \
            --latency-threshold 50 \
            --model-size-limit 10
      
      - name: Upload AI test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: ai-test-results-${{ matrix.ai-component }}
          path: |
            test-results/
            model-performance-reports/

  regression-tests:
    runs-on: ubuntu-latest
    needs: [unit-tests, performance-tests, cross-platform-tests]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          submodules: recursive
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r tests/requirements-test.txt
      
      - name: Download artifacts from previous tests
        uses: actions/download-artifact@v4
        with:
          path: previous-test-results/
      
      - name: Run regression tests
        run: |
          python -m pytest tests/ui_ux_comprehensive/regression/ \
            --junit-xml=test-results/regression-tests.xml \
            --baseline-dir previous-test-results/ \
            -v \
            --tb=short
      
      - name: Check for performance regressions
        run: |
          python tests/ui_ux_comprehensive/detect_regressions.py \
            --current-results test-results/ \
            --baseline-results baseline-results/ \
            --threshold 10
      
      - name: Upload regression results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: regression-test-results
          path: test-results/

  # Comprehensive Results Analysis
  analyze-results:
    runs-on: ubuntu-latest
    needs: [
      unit-tests,
      performance-tests,
      cross-platform-tests,
      accessibility-tests,
      web-interface-tests,
      ai-intelligence-tests,
      regression-tests
    ]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          pip install -r tests/requirements-test.txt
          pip install jinja2 matplotlib seaborn pandas
      
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-test-results/
      
      - name: Generate comprehensive test report
        run: |
          python tests/ui_ux_comprehensive/generate_comprehensive_report.py \
            --results-dir all-test-results/ \
            --output comprehensive-test-report.html \
            --include-performance-graphs \
            --include-coverage-analysis \
            --include-regression-analysis
      
      - name: Validate overall test success
        run: |
          python tests/ui_ux_comprehensive/validate_test_success.py \
            --results-dir all-test-results/ \
            --performance-threshold 85 \
            --coverage-threshold 85 \
            --accessibility-threshold 95
      
      - name: Generate performance baseline
        if: github.ref == 'refs/heads/main'
        run: |
          python tests/ui_ux_comprehensive/generate_baseline.py \
            --results-dir all-test-results/ \
            --output-dir baseline-results/
      
      - name: Upload comprehensive results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: comprehensive-test-results
          path: |
            comprehensive-test-report.html
            baseline-results/
            test-summary.json
      
      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const testSummary = JSON.parse(fs.readFileSync('test-summary.json', 'utf8'));
            
            const comment = `## 🧪 Comprehensive UI/UX Test Results
            
            **Overall Status**: ${testSummary.success ? '✅ PASSED' : '❌ FAILED'}
            
            ### Test Summary
            - **Unit Tests**: ${testSummary.unit_tests.passed}/${testSummary.unit_tests.total} passed (${testSummary.unit_tests.success_rate}%)
            - **Performance Tests**: ${testSummary.performance_tests.fps_target_met ? '✅' : '❌'} 60fps target, ${testSummary.performance_tests.memory_under_limit ? '✅' : '❌'} <200MB memory
            - **Cross-platform**: ${testSummary.cross_platform.platforms_passed}/${testSummary.cross_platform.total_platforms} platforms passed
            - **Accessibility**: ${testSummary.accessibility.wcag_compliance}% WCAG AA compliance
            - **Web Interface**: ${testSummary.web_interface.browsers_passed}/${testSummary.web_interface.total_browsers} browsers passed
            - **AI Intelligence**: ${testSummary.ai_intelligence.model_accuracy}% average accuracy
            
            ### Performance Metrics
            - **Animation FPS**: ${testSummary.performance.average_fps} fps (target: 60fps)
            - **Memory Usage**: ${testSummary.performance.peak_memory_mb} MB (target: <200MB)
            - **Test Coverage**: ${testSummary.coverage.percentage}% (target: 85%)
            
            ${testSummary.regressions_detected ? '⚠️ **Performance regressions detected**' : '✅ No performance regressions'}
            
            [View full report](${testSummary.report_url})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Deployment to test environments
  deploy-test-environment:
    runs-on: ubuntu-latest
    needs: analyze-results
    if: github.ref == 'refs/heads/main' && success()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Deploy to test environment
        run: |
          echo "Deploying to test environment with validated UI/UX components"
          # Deployment steps would go here
      
      - name: Run smoke tests in deployed environment
        run: |
          python tests/ui_ux_comprehensive/smoke_tests.py \
            --environment test \
            --validate-60fps \
            --validate-memory-usage \
            --validate-accessibility
      
      - name: Update performance baselines
        run: |
          python tests/ui_ux_comprehensive/update_baselines.py \
            --source all-test-results/ \
            --target baseline-results/ \
            --commit-changes

# Workflow-level configuration
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Environment variables for all jobs
env:
  FORCE_COLOR: 1
  PYTEST_XDIST_WORKER_COUNT: auto
  COMPREHENSIVE_TESTING: true